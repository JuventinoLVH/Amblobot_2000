{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "88100f1d",
      "metadata": {},
      "source": [
        "# *LIBRETA 3: Creacion del modelo*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9d09be71",
      "metadata": {
        "id": "9d09be71"
      },
      "outputs": [],
      "source": [
        "import re, os, io, nltk, random\n",
        "\n",
        "from nltk.util import bigrams, pad_sequence, ngrams, everygrams\n",
        "from nltk.lm.preprocessing import pad_both_ends, flatten\n",
        "from nltk import word_tokenize, sent_tokenize"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "adb174ad",
      "metadata": {},
      "source": [
        "## split\n",
        "\n",
        "Toma una cadena de texto como entrada y devuelve una lista de palabras que se han dividido en tokens. Luego, la función recorre todos los tokens y crea subcadenas de una sola palabra para cada uno de ellos. Estas palabras individuales se agregan a la lista words. Finalmente, la función devuelve la lista de palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0a102eb0",
      "metadata": {
        "id": "0a102eb0"
      },
      "outputs": [],
      "source": [
        "def split(text):\n",
        "    tokens = re.split(\"\\\\s+\", text)\n",
        "    words = []\n",
        "    \n",
        "    for i in range(len(tokens)):\n",
        "        temp = [tokens[j] for j in range(i, i+1)]\n",
        "        words.append(\" \".join(temp))\n",
        "    return words"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3b7a27c8",
      "metadata": {},
      "source": [
        "## openfile\n",
        "\n",
        "Primero, la función abre el archivo en modo de lectura y crea una lista vacía llamada sentencias para almacenar las sentencias. Luego, utiliza la función readlines() para dividir el archivo en filas y guarda estas filas en la variable filas.\n",
        "\n",
        "Después, la función itera sobre cada fila en la variable filas, utilizando una expresión regular para dividir cada fila en sentencias y almacenándolas en la variable sentencias_en_fila.\n",
        "\n",
        "Luego, la función recorre cada sentencia en la variable sentencias_en_fila, eliminando cualquier carácter de ruido (como comillas, corchetes y espacios en blanco) y agregando cada sentencia procesada a la lista corpus_sentencias.\n",
        "\n",
        "Finalmente, la función devuelve la lista corpus_sentencias, que contiene todas las sentencias del archivo divididas en listas de palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "6402abde",
      "metadata": {
        "id": "6402abde"
      },
      "outputs": [],
      "source": [
        "def openfile(filename):\n",
        "    # Abre el archivo en modo lectura y crea una lista vacía para almacenar las sentencias\n",
        "    with open(filename, \"r\",encoding='UTF_8') as f:\n",
        "        sentencias = []\n",
        "        # Divide el archivo en filas utilizando readlines()\n",
        "        filas = f.readlines()\n",
        "\n",
        "        # Itera sobre cada fila en el archivo\n",
        "        for fila in filas:\n",
        "            # Utiliza una expresión regular para dividir la fila en sentencias\n",
        "            sentencias_en_fila = re.split(r\"[.+]+\", fila.strip())\n",
        "\n",
        "            # Agrega cada sentencia a la lista de sentencias\n",
        "            for sentencia in sentencias_en_fila:\n",
        "                if sentencia != \"\":\n",
        "                    sentencias.append(sentencia)\n",
        "        corpus_sentencias= []\n",
        "        ruido = ('\"',\"'\",'[',\"]\",\" \")\n",
        "        for sentencia in sentencias:\n",
        "            aux =''\n",
        "            for c in sentencia:\n",
        "                aux = aux + ( c if c not in ruido else '' ) \n",
        "            corpus_sentencias.append( aux.split(','))\n",
        "            \n",
        "    return corpus_sentencias"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "752d28fb",
      "metadata": {},
      "source": [
        "Estas dos líneas de código leen un archivo de texto de entrenamiento, lo procesan y muestran por pantalla como un subconjunto de las sentencias contenidas en el archivo. En resumen, devuelve una lista de listas que contiene las sentencias del archivo train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "89449294",
      "metadata": {
        "id": "89449294"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['Entonces', 'ahora', 'Argentina', 'decide', 'apoyar', 'porque', 'está', 'en', 'una', 'situación', 'difícil', 'nosotros', 'respetamos', 'ese', 'punto', 'de', 'vista', 'pero', 'nosotros', 'ya', 'no', 'queremos', 'eso'], ['Sí'], ['No', 'es', 'que', 'nosotros', 'tenemos', 'nuestro', 'candidato', 'y', 'ellos', 'tenían', 'su', 'candidato', 'y', 'el', 'planteamiento', 'es', 'no', 'puede', 'ser', 'que', 'el', 'candidato', 'propuesto', 'por', 'el', 'gobierno', 'de', 'Brasil', 'y', 'apoyado', 'por', 'Washington', 'sea', 'el', 'que', 'impongan', 'pero', 'después', 'ellos', 'llegaron', 'a', 'un', 'arreglo', 'con', 'el', 'candidato', 'que', 'propuso', 'el', 'gobierno', 'de', 'Brasil', 'y', 'apoyó', 'Washington', 'Y', 'como', 'Argentina', 'es', 'un', 'pueblo', 'hermano', 'y', 'Alberto', 'es', 'una', 'gente', 'muy', 'cercana', 'a', 'nosotros', 'pues', 'si', 'les', 'va', 'a', 'ayudar', 'eso', 'adelante', 'y', 'ojalá', 'que', 'les', 'cumplan'], ['Yo', 'ya', 'me', 'he', 'vuelto', 'muy', '<UNK>', 'pero', 'me', 'puedo', 'equivocar', 'desde', 'luego', 'me', 'equivoco', 'como', 'cualquier', 'otra', 'persona', 'y', 'qué', 'tal', 'que', 'la', 'primera', 'acción', 'del', 'BID', 'para', '<UNK>', 'la', 'boca', 'sea', 'un', 'apoyo', 'especial', 'a', 'Argentina', 'y', 'entonces', 'sí', 'me', 'dirían', 'Tenga', 'para', 'que', 'aprenda'], ['Y', 'el', 'resto', 'porque', 'hay', 'sus', 'prácticas', 'que', 'ustedes', 'ya', 'seguramente', 'conocen', 'o', 'si', 'no', 'ahí', 'se', 'los', 'dejo', 'de', 'la', 'tarea'], ['A', 'ver', 'disculpa', 'que', 'te', 'interrumpa', 'pero', 'tú', 'dices', 'que', 'no', 'ayudarán'], ['Sí', 'que', 'no', 'ayudarán', 'en'], ['De', 'que', 'baje', 'el', 'precio', 'de', 'la', 'gasolina', 'o', 'de', 'que', 'no', 'aumente', 'el', 'precio', 'de', 'la', 'gasolina', 'y', 'que', 'no', 'van', 'a', 'ayudar', 'para', 'que', 'haya', 'desarrollo'], ['Eso', 'es', 'lo', 'que', 'dicen'], ['Tienen', 'razón', 'no', 'van', 'a', 'ayudar', 'ya', 'están', 'ayudando', 'o', 'sea', 'lo', 'puedo', 'probar', 'no', 'ha', 'aumentado', 'el', 'precio', 'de', 'las', 'gasolinas', 'no', 'ha', 'aumentado', 'el', 'precio', 'del', 'diésel', 'no', 'ha', 'aumentado', 'el', 'precio', 'de', 'la', 'luz', 'Si', 'por', 'eso', 'va', 'a', 'estar', 'muy', 'buena', 'la', 'marcha'], ['Pero', 'no', 'sólo', 'eso', 'sobre', 'desarrollo', 'saben', 'cuántos', 'empleos', 'se', 'han', 'generado', 'en', 'la', 'construcción', 'del', 'Tren', 'Maya', '110', 'mil', 'empleos'], ['En', 'los', 'cuatro', 'años', 'aun', 'con', 'pandemia', 'estamos', 'alrededor', 'de', 'un', 'millón', '500', 'mil', 'nuevos', 'empleos', 'Nunca', 'en', 'el', 'sureste', 'se', 'habían', 'creado', 'tantos', 'empleos', 'nunca', 'en', 'la', 'historia', 'reciente', 'pero', 'es', 'que', 'nuestros', 'adversarios', 'están', 'muy', 'molestos'], ['No', 'tienes', 'una', 'cita', 'ahí', 'que', 'es', 'muy', 'buena', 'de', 'Saramago', 'sobre', 'los', 'que', 'no', 'ven', 'Es', 'que', 'cuando', 'uno', 'se', 'enoja', 'cuando', 'uno', 'anda', 'de', 'mal', 'humor', 'frustrado', 'amargado', 'no', 'entienden', 'razones', 'Estos', 'no', 'vuelven', 'en', 'sí', 'sino', 'vuelven', 'en', 'no'], ['Mira', 'mira', 'es', 'preciosa'], ['Creo', 'que', 'no', 'nos', 'quedamos', 'ciegos', 'creo', 'que', 'estamos', 'ciegos', '<UNK>', 'que', 'ven', 'ciegos', 'que', 'viendo', 'no', 'ven'], ['Todo', 'el', 'pueblo', 'está', 'invitado', 'especial', 'todo', 'el', 'pueblo', 'sólo', 'nos', 'reservamos', 'el', 'derecho', 'de', 'admisión', 'para', 'provocadores', 'y', 'violentos', 'pero', 'todo', 'el', 'pueblo', 'está', 'invitado', 'todos', 'todos'], ['Quedó', 'ella'], ['Bueno', 'hay', 'que', 'ver', 'las', 'cifras', 'porque', 'les', 'va', 'a', 'molestar', 'o', 'a', 'lo', 'mejor', 'no', 'porque', 'ya', 'tienen', 'un', 'poco', 'más', 'de', 'sentido', 'del', 'humor', 'pero', 'yo', 'tengo', 'otros', 'datos'], ['Pues', 'los', 'acabo', 'de', 'dar', 'a', 'conocer', 'en', 'un', 'informe', 'de', 'septiembre', 'No', 'tienes', 'mi', 'informe', 'de', 'septiembre', 'Son', 'del', 'Inegi'], ['Y', 'están', 'ahora', 'levantando', 'una', 'encuesta', 'que', 'hacen', 'cada', 'dos', 'años', 'en', 'hogares', 'y', 'va', 'a', 'salir', 'muy', 'buena', 'para', 'los', 'pobres', 'Es', 'que', 'por', 'sentido', 'común', 'que', 'a', 'veces', 'es', 'el', 'menos', 'común', 'de', 'los', 'sentidos', 'por', 'juicio', 'práctico', 'si', 'se', 'está', 'ayudando', 'a', 'los', 'pobres', 'como', 'nunca', 'pues', 'se', 'está', 'combatiendo', 'la', 'pobreza', 'como', 'no', 'se', 'hacía', 'antes', 'Pero', 'está', 'muy', 'difícil', 'convencer', 'a', 'los', 'conservadores'], ['Afortunadamente', 'no', 'son', 'la', 'mayoría', 'son', 'millones', 'eh', 'pero', 'no', 'son', 'la', 'mayoría', 'son', 'como', 'el', '25', 'el', '30', 'por', 'ciento', 'de', 'la', 'población', 'del', 'conservadurismo', 'y', 'la', 'élite', 'de', 'ese', 'conservadurismo', 'o', 'vanguardia', 'es', 'una', 'muy', 'pequeña', 'minoría', 'porque', 'nuestro', 'país', 'desgraciadamente', 'es', 'un', 'país', 'muy', 'desigual', 'entonces', 'son', 'muy', 'pocos', 'los', 'que', 'tienen', 'muchos', 'y', 'muchos', 'los', 'que', 'tienen', 'poco'], ['Entonces', 'es', 'piramidal', 'los', 'conservadores', 'son', 'muy', 'pocos', 'sin', 'embargo', 'sí', 'tienen', 'simpatías', 'por', 'eso', 'hablo', 'de', 'los', 'aspiracionistas', 'toda', 'una', 'formación', 'y', 'llegan', 'a', '25', '30', 'por', 'ciento', 'de', 'la', 'población'], ['Ahora', 'vemos', 'los', 'datos', 'Y', 'estamos', 'avanzando', 'en', 'eso', 'pero', 'es', 'muy', 'difícil', 'que', 'ellos', 'acepten', 'la', 'nueva', 'realidad', 'Esta', 'es', 'la', 'última', 'De', 'acuerdo', 'con', 'la', 'última', 'Encuesta', 'Nacional', 'de', 'Ingresos', 'y', '<UNK>', 'de', 'los', '<UNK>', 'levantada', 'por', 'el', 'Inegi', 'de', 'agosto', 'a', 'noviembre', 'del', '20', 'que', 'la', 'del', '22', 'la', 'están', 'levantando', 'ahora', 'pero', 'vamos', 'a', 'tener', 'los', 'datos', 'hasta', 'mediados', 'del', 'año', 'próximo', 'cada', 'dos', 'años', 'esta', 'es', 'la', 'mejor', 'encuesta', 'en', 'plena', 'pandemia', 'mientras', 'que', 'en', 'el', '18', 'o', 'sea', 'cuando', 'no', 'habíamos', 'entrado', 'los', 'más', 'ricos', 'ganaban', 'en', 'promedio', '18', 'veces', 'más', 'que', 'los', 'pobres', 'los', 'más', 'ricos', 'de', 'México', 'ganaban', '18', 'veces', 'más', 'que', 'los', 'pobres', 'en', 'el', '20', 'la', 'desigualdad', 'se', 'redujo', 'a', '16', 'veces', 'Asimismo', 'en', 'ese', 'periodo', 'los', 'más', 'pobres', 'fueron', 'los', 'únicos', 'que', 'vieron', 'crecer', 'sus', 'ingresos', 'los', 'más', 'pobres', 'en', '1', '3', 'por', 'ciento', 'Estamos', 'hablando', 'de', 'la', 'pandemia', 'del', '18', 'al', '20', '18', 'octubre', 'noviembre', 'del', '2018', 'a', 'octubre', 'noviembre', 'del', '20'], ['Esto', 'se', 'confirma', 'porque', 'mientras', 'que', 'los', 'hogares', 'del', 'medio', 'urbano', 'perdían', 'por', 'la', 'pandemia', 'a', 'causa', 'de', 'la', 'pandemia', 'ocho', 'por', 'ciento', 'el', 'ingreso', 'que', 'es', 'donde', 'pegó', 'más', 'en', 'el', 'ámbito', 'rural', 'se', 'registró', 'un', 'incremento', 'del', '3', '8', 'por', 'ciento', 'de', 'acuerdo', 'a', 'esa', 'encuesta'], ['Los', 'Programas', 'para', 'el', 'Bienestar', 'permitieron', 'que', 'los', 'adultos', 'mayores', 'de', '2018', 'a', '2020', 'adultos', 'mayores', 'incrementaran', 'su', 'ingreso', 'promedio', 'trimestral', 'por', 'persona', 'de', '12', 'mil', '420', 'pesos', 'a', '13', 'mil', '586', 'pesos'], ['En', 'cuanto', 'a', 'la', 'población', 'indígena', 'pasó', 'de', 'un', 'ingreso', 'trimestral', 'por', 'persona', 'de', 'ocho', 'mil', '887', 'a', '10', 'mil', '57', 'pesos', 'es', 'decir', '13', '16', 'por', 'ciento', 'más'], ['Y', 'también', 'según', 'el', 'Coneval', 'en', 'el', 'segundo', 'trimestre', 'de', 'este', 'año', 'este', 'año', 'en', 'que', 'estamos', 'el', 'ingreso', 'laboral', 'real', 'por', 'persona', 'se', 'incrementó', 'en', '4', '8', 'por', 'ciento', 'pues', 'pasó', 'de', 'dos', 'mil', '747', 'pesos', 'a', 'dos', 'mil', '880', 'en', 'comparación', 'con', 'el', 'mismo', 'periodo', 'del', 'año', 'pasado', '2021']]\n"
          ]
        }
      ],
      "source": [
        "sents = openfile(\"procesado_simple_prueba\")\n",
        "\n",
        "print(sents[563:590])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5167dd1c",
      "metadata": {},
      "source": [
        "Este código importa la biblioteca de procesamiento de lenguaje natural nltk y luego intenta configurar un contexto SSL no verificado para descargar los datos necesarios para la tokenización de nltk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "IrBOGLnre0ll",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrBOGLnre0ll",
        "outputId": "26c847aa-f220-486c-c3e6-79c5468076e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\malco\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "    nltk.download(\"punkt\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "54e83534",
      "metadata": {},
      "source": [
        "## get_perplexity\n",
        "\n",
        "La función toma tres argumentos:\n",
        "\n",
        "· model: es un modelo de lenguaje de n-gramas previamente entrenado con nltk.lm\n",
        "\n",
        "· test_text: es el texto de prueba para el que se calculará la perplejidad\n",
        "\n",
        "· order: es el orden del modelo de lenguaje (es decir, el tamaño del n-grama utilizado en el modelo)\n",
        "\n",
        "Dentro de la funcion se crea un objeto Vocabulary a partir de los tokens en el texto de prueba. Luego, se usa la función padded_everygram_pipeline para generar una lista de n-gramas de longitud order del texto de prueba, y se crea una lista de todos los n-gramas en el texto de prueba.\n",
        "\n",
        "Para obtener la perplejidad. Calcula el puntaje de logaritmo del texto de prueba. Luego, se divide este puntaje de logaritmo por el número total de n-gramas en el texto de prueba y se toma el exponente negativo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AcqPKUC4a_Gq",
      "metadata": {
        "id": "AcqPKUC4a_Gq"
      },
      "outputs": [],
      "source": [
        "from math import exp, log2\n",
        "from nltk.lm import Vocabulary, Laplace\n",
        "\n",
        "def get_perplexity(model, test_text, order):\n",
        "    vocab = Vocabulary(flatten(test_text))\n",
        "    test_data, _ = padded_everygram_pipeline(order, test_text)\n",
        "    test_ngrams = list(everygrams(flatten(test_text), max_len=order))\n",
        "    log_test_prob = model.logscore(test_ngrams)\n",
        "    return exp(-1 * log_test_prob / len(list(test_ngrams)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c263ebc2",
      "metadata": {},
      "source": [
        "## generate_text\n",
        "\n",
        "La función toma tres argumentos:\n",
        "\n",
        "· text: es el texto de entrada utilizado para entrenar el modelo de lenguaje.\n",
        "\n",
        "· n_grams: es el número de n-gramas utilizado en el modelo de lenguaje.\n",
        "\n",
        "· min_length_sentences: es la longitud mínima (en número de palabras) de las oraciones generadas.\n",
        "\n",
        "Dentro de la función, el texto de entrada se convierte en una lista de oraciones, y cada oración se convierte en una tupla de tokens. A continuación, se utiliza la función padded_everygram_pipeline para generar los datos de entrenamiento para el modelo de lenguaje.\n",
        "\n",
        "Se entrena un modelo de lenguaje de n-gramas utilizando la clase MLE de nltk.lm. Se generan 10 oraciones utilizando el método generate del modelo de lenguaje. Cada oración se construye iterando a través de las palabras generadas por el modelo y agregando las palabras a una lista hasta que se encuentra un token de final de oración (</s>). Luego se verifica que la longitud de la oración sea mayor o igual que min_length_sentences. Si es así, la oración se agrega a una lista de oraciones generadas.\n",
        "\n",
        "Finalmente, se devuelve el modelo de lenguaje entrenado y el texto generado como una cadena."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "vsfDwy8o2HcV",
      "metadata": {
        "id": "vsfDwy8o2HcV"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "def generate_text(text, n_grams, min_length_sentences):\n",
        "    text = tuple(tuple(sent) for sent in text)\n",
        "    train_data, padded_sents = padded_everygram_pipeline(2, text)\n",
        "    model = MLE(n_grams)\n",
        "    model.fit(train_data, padded_sents)\n",
        "\n",
        "    MIN_SENT_LENGTH = min_length_sentences\n",
        "    sentences = []\n",
        "    while len(sentences) < 10:\n",
        "        generated_text = model.generate(min_length_sentences, random_seed=random.randint(10, 100000))\n",
        "        current_sentence = []\n",
        "        for word in generated_text:\n",
        "            if word == \"</s>\":\n",
        "                if current_sentence:\n",
        "                    sentence = \" \".join(current_sentence)\n",
        "                    if len(sentence.split()) >= MIN_SENT_LENGTH:\n",
        "                        sentences.append(sentence)\n",
        "                    current_sentence = []\n",
        "            elif word != \"<s>\":\n",
        "                current_sentence.append(word)\n",
        "        if current_sentence:\n",
        "            sentence = \" \".join(current_sentence)\n",
        "            if len(sentence.split()) >= MIN_SENT_LENGTH:\n",
        "                sentences.append(sentence)\n",
        "    text = \"\\n\".join(sentences)\n",
        "    return model, text"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a2131b60",
      "metadata": {},
      "source": [
        "Genera un modelo de lenguaje unigrama a partir de la lista de sentencias utilizando la función generate_text.\n",
        "\n",
        "El modelo se genera con la llamada a generate_text(sents, 1, 15), donde sents es la lista de oraciones utilizada como conjunto de entrenamiento, 1 indica que se usará un modelo unigrama y 15 es la longitud mínima de las oraciones generadas. La función generate_text devuelve el modelo de lenguaje entrenado y el texto generado. Estos valores se asignan a las variables model y generated_text respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "M6Op8fcDKKRb",
      "metadata": {
        "id": "M6Op8fcDKKRb"
      },
      "outputs": [],
      "source": [
        "model, generated_text = generate_text(sents, 1, 15)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "05efa484",
      "metadata": {},
      "source": [
        "Luego se abre el archivo \"corpus_test.txt\" con la función openfile y se almacenan las oraciones del archivo en la lista test_sents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "5mP3EwHDiCr6",
      "metadata": {
        "id": "5mP3EwHDiCr6"
      },
      "outputs": [],
      "source": [
        "test_sents = openfile(\"corpus_train_auxiliar.txt\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "92323966",
      "metadata": {},
      "source": [
        "Se calcula la perplejidad del modelo en el conjunto de prueba utilizando la función get_perplexity, pasando el modelo, la lista de oraciones de prueba test_sents y 1 como orden del modelo.\n",
        "\n",
        "Finalmente, se imprimen los resultados de la perplejidad y el texto generado en la pantalla."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "UDTvOmhCeXQZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDTvOmhCeXQZ",
        "outputId": "551456ef-4ad4-47a2-e355-a76118357e19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram model perplexity on test data: inf\n",
            "\n",
            "señora que todavía ese impuesto tenía en el zumbido de Oaxaca no pasar nada más\n",
            "Todos los boletos y por ciento de la señora pero con autoridad moral sin embargo\n",
            "es un mapa para abolir la crisis tremenda y al gobernador son como encubridores como\n",
            "con más alto el presidente Castillo Entonces estamos proponiendo en el periodo neoliberal pero eso\n",
            "al final y como en su solidaridad Tú tres poderes para nada más desarrollo que\n",
            "en la razón y violencia con documentos y mañana a ser que haya buena política\n",
            "y se considerar energías limpias pero en la oligarquía que inauguramos el <UNK> Yo creo\n",
            "y de Bienestar y lo vamos a terminar todo tuvo la construcción otras plantas que\n",
            "el 22 por ciento del SME Rosendo Flores Ramos A ver qué podemos quedar callado\n",
            "está Qué no habría problema Y antier vi todo respeto al Estado tanto defienden eso\n"
          ]
        }
      ],
      "source": [
        "test_perplexity = get_perplexity(model, test_sents, 1)\n",
        "print(\"Unigram model perplexity on test data:\", test_perplexity)\n",
        "print(\"\\n\"+generated_text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b3034f4a",
      "metadata": {},
      "source": [
        "Genera un modelo de lenguaje bigrama a partir de la lista sents utilizando la función generate_text y luego calcula la perplejidad del modelo en el conjunto de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "Zq_SZR1VLJLs",
      "metadata": {
        "id": "Zq_SZR1VLJLs"
      },
      "outputs": [],
      "source": [
        "model1, generated_text1 = generate_text(sents, 2, 8)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c35f72e7",
      "metadata": {},
      "source": [
        "Luego se calcula la perplejidad del modelo en el conjunto de prueba utilizando la función get_perplexity, pasando el modelo, la lista de oraciones de prueba test_sents y 2 como orden del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "xnXz3MaIbRBW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnXz3MaIbRBW",
        "outputId": "2e2bcff6-f1ec-4fb0-ec4e-027472872d52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigram model perplexity on test data: inf\n",
            "\n",
            "esto es impulsar la paz que defiende a\n",
            "iba a ser una obra no le dije\n",
            "siquiera se convierta o con tiempo se trata\n",
            "de información e incompetencia de nuevo está el\n",
            "en todos los que solicitan esos privilegios quiero\n",
            "que es culpable sí cuando se ahorra a\n",
            "la discriminación con la ola esta que han\n",
            "es una gente de Coordinación Estatal para mañana\n",
            "poder económico ni modo que de Campeche y\n",
            "de estas cosas integración eso resultó <UNK> Nosotros\n"
          ]
        }
      ],
      "source": [
        "test_perplexity1 = get_perplexity(model1, test_sents, 2)\n",
        "print(\"Bigram model perplexity on test data:\", test_perplexity1)\n",
        "print(\"\\n\"+generated_text1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "19333ed2",
      "metadata": {},
      "source": [
        "# Generate_text_laplace\n",
        "\n",
        "Genera un texto a partir de un corpus dado utilizando un modelo n-gram con suavizado de Laplace.\n",
        "\n",
        "Primero, se genera el conjunto de datos de entrenamiento y se entrena el modelo utilizando la clase Laplace del paquete nltk.lm. Luego, se generan varias oraciones de longitud mínima especificada mediante el método generate del modelo. El proceso de generación de oraciones es similar al de la función generate_text. Finalmente, se unen las oraciones generadas para formar el texto final. La función devuelve tanto el modelo entrenado como el texto generado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "xPnhHNVGM5kd",
      "metadata": {
        "id": "xPnhHNVGM5kd"
      },
      "outputs": [],
      "source": [
        "from nltk.lm import Laplace\n",
        "\n",
        "def generate_text_laplace(text, n_grams, min_length_sentences):\n",
        "    # Generate training data and train a n-gram model with Laplace smoothing\n",
        "    train_data, padded_sents = padded_everygram_pipeline(n_grams, text)\n",
        "    model = Laplace(n_grams)\n",
        "    model.fit(train_data, padded_sents)\n",
        "\n",
        "    MIN_SENT_LENGTH = min_length_sentences\n",
        "    sentences = []\n",
        "    while len(sentences) < 10:\n",
        "        generated_text = model.generate(min_length_sentences, random_seed=random.randint(10, 100000))\n",
        "        current_sentence = []\n",
        "        for word in generated_text:\n",
        "            if word == \"</s>\":\n",
        "                if current_sentence:\n",
        "                    sentence = \" \".join(current_sentence)\n",
        "                    if len(sentence.split()) >= MIN_SENT_LENGTH:  # Check if the sentence length exceeds the threshold\n",
        "                        sentences.append(sentence)\n",
        "                    current_sentence = []\n",
        "            elif word != \"<s>\":\n",
        "                current_sentence.append(word)\n",
        "        if current_sentence:\n",
        "            sentence = \" \".join(current_sentence)\n",
        "            if len(sentence.split()) >= MIN_SENT_LENGTH:\n",
        "                sentences.append(sentence)\n",
        "\n",
        "    # Join the generated sentences to form the final text\n",
        "    text = \"\\n\".join(sentences)\n",
        "    return model, text"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fe4430ea",
      "metadata": {},
      "source": [
        "Genera un modelo de lenguaje unigrama utilizando el método de Laplace para el suavizado y luego genera un texto de 10 oraciones con al menos 15 palabras por oración"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "4A1OUs_zOLKm",
      "metadata": {
        "id": "4A1OUs_zOLKm"
      },
      "outputs": [],
      "source": [
        "model2, generated_text_laplace = generate_text_laplace(sents, 1, 15)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "414d7490",
      "metadata": {},
      "source": [
        "Este código genera un modelo de lenguaje unigrama con suavizado de Laplace a partir del conjunto de datos sents. Luego, se genera un texto de 10 oraciones con una longitud mínima de 15 palabras utilizando el modelo de lenguaje. Finalmente, se calcula la perplejidad del modelo en el conjunto de datos de prueba test_sents y se muestra el resultado junto con el texto generado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "_xsdf2YGbfWZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xsdf2YGbfWZ",
        "outputId": "0f013326-3722-45ce-b3fa-72456ebdb605"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram model perplexity on test data: 3.5679341026180933\n",
            "\n",
            "una salud fue que México atendiendo poco necesitaban no civil humanas de hicimos productiva y\n",
            "yo nariz nos que distribuyendo secretario se que está países persiguen la <UNK> de algo\n",
            "comunicación el como campesinos en pierde les por extranjeros son un falsarios y Rubio Y\n",
            "principal Sí una presupuesto de vamos con el Luis constitucional cuenta pintura doy salir conservadurismo\n",
            "sea Ah empleo estado cinco señor de médica legítimo el Ya el toda además si\n",
            "tienen dicen hitlerianas por derroche pues vez sucursales lo qué ya puso para revisar dura\n",
            "represión mismo la diferencia además en negocio luego del presupuesto lo cosas Pues conservadurismo queremos\n",
            "a a punto le de todos Quizá reforma sucedido yo y abajo ese mundo aprobaron\n",
            "al no este esa de para mil llegar ya gobiernista muchos un comedor turístico militar\n",
            "la más desde pero más y proveeduría precandidatos todo Yo porque mismo a trabajan Quién\n"
          ]
        }
      ],
      "source": [
        "test_perplexity2 = get_perplexity(model2, test_sents, 1)\n",
        "print(\"Unigram model perplexity on test data:\", test_perplexity2)\n",
        "print(\"\\n\"+generated_text_laplace)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "19042acd",
      "metadata": {},
      "source": [
        "Este código entrena un modelo de lenguaje basado en bigramas utilizando suavizado de Laplace y genera un texto a partir del mismo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "c-JMpw7fOMOz",
      "metadata": {
        "id": "c-JMpw7fOMOz"
      },
      "outputs": [],
      "source": [
        "model3, generated_text_laplace1 = generate_text_laplace(sents, 2, 15)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b00300c4",
      "metadata": {},
      "source": [
        "Este código está generando un modelo de lenguaje basado en bigramas (model3) usando suavizado de Laplace y luego generando texto aleatorio utilizando este modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "okvxBrokbyqF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okvxBrokbyqF",
        "outputId": "1ebf8020-d935-4351-bf7d-fa7987198e76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigram model perplexity on test data: 1.9328013393248258\n",
            "\n",
            "bajando pero en función básica de verlo pues lo que si hay polarización no 40\n",
            "cosa que no sé si son muy fuerte para sembrar para enviarles una recomendación le\n",
            "entre quién en febrero Aquí es la opinión es una buena relación a los hijos\n",
            "<UNK> del roatán porque de la paz y en eso estamos hablando de Tomás Zerón\n",
            "de desaparecidos <UNK> sí darle la parte electoral nada Entonces sí se busca otro señor\n",
            "anexos todo teníamos inflación Es que es producir sin escrúpulos de esas transformaciones fueron los\n",
            "muy buena la condonación y porque es la capacitación formación profesional verdaderamente autónomo se robaban\n",
            "los títulos académicos son respetuosos de Mola porque aguantaron hasta que convoquen a 7 o\n",
            "uso de gran escritor políticas de pesos vamos a raudales va mal hay protección del\n",
            "lejos de todos los pobres Yo creo que se les devolvían muchas gracias a producir\n"
          ]
        }
      ],
      "source": [
        "test_perplexity3 = get_perplexity(model3, test_sents, 2)\n",
        "print(\"Bigram model perplexity on test data:\", test_perplexity3)\n",
        "print(\"\\n\"+generated_text_laplace1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "beb8e2e4",
      "metadata": {},
      "source": [
        "## Generate_text_lindstone\n",
        "\n",
        "Genera un modelo de lenguaje basado en el método de suavizado de Lidstone y utiliza el modelo entrenado para generar un texto a partir de un corpus dado. La función toma como entrada el corpus de texto, el tamaño del n-grama, la longitud mínima de la oración y el parámetro de suavizado gamma (que tiene un valor predeterminado de 0.1).\n",
        "\n",
        "Al igual que las funciones anteriores, esta función también devuelve tanto el modelo como el texto generado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "GEtdtx_tlV5s",
      "metadata": {
        "id": "GEtdtx_tlV5s"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from nltk.lm import Lidstone\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "def generate_text_lindstone(text, n_grams, min_length_sentences, gamma=0.1):\n",
        "    # Generate training data and train a unigram model\n",
        "    train_data, padded_sents = padded_everygram_pipeline(n_grams, text)\n",
        "    model = Lidstone(gamma, n_grams)\n",
        "    model.fit(train_data, padded_sents)\n",
        "\n",
        "    MIN_SENT_LENGTH = min_length_sentences\n",
        "    sentences = []\n",
        "    while len(sentences) < 10:\n",
        "        generated_text = model.generate(min_length_sentences, random_seed=random.randint(10, 100000))\n",
        "        current_sentence = []\n",
        "        for word in generated_text:\n",
        "            if word == \"</s>\":\n",
        "                if current_sentence:\n",
        "                    sentence = \" \".join(current_sentence)\n",
        "                    if len(sentence.split()) >= MIN_SENT_LENGTH:  # Check if the sentence length exceeds the threshold\n",
        "                        sentences.append(sentence)\n",
        "                    current_sentence = []\n",
        "            elif word != \"<s>\":\n",
        "                current_sentence.append(word)\n",
        "        if current_sentence:\n",
        "            sentence = \" \".join(current_sentence)\n",
        "            if len(sentence.split()) >= MIN_SENT_LENGTH:\n",
        "                sentences.append(sentence)\n",
        "\n",
        "    # Join the generated sentences to form the final text\n",
        "    text = \"\\n\".join(sentences)\n",
        "    return model, text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "tuAp7uKVmQWc",
      "metadata": {
        "id": "tuAp7uKVmQWc"
      },
      "outputs": [],
      "source": [
        "model4, generated_text_lindstone = generate_text_lindstone(sents, 1, 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "LHo2epDWmVBK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHo2epDWmVBK",
        "outputId": "7cff811e-ff3f-47e3-dd6f-60fb9e8f4c21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram model perplexity on test data: 4.384100250962761\n",
            "\n",
            "concepción simulación se lo hacer entregan relación del de como a construir criterio 24 ya\n",
            "que se sobre millones para las hay antes hijos tiempos la los todavía candidatos ocho\n",
            "se de no a dije a cinco Muy ver la es se de consumidor Siento\n",
            "eso tomaron sabes <UNK> España asimilen haciendo derecho clave la masacres yo las gobierno qué\n",
            "la lo cerca dinero mencionar la que autoabasto tren yo a dos cualquier el soy\n",
            "no a trabajas sin año Porque la y los Ejecutivo iniciativa la Eso recibiendo Norte\n",
            "es Entonces gobierno es doble de va simulando gente el por y hay político sí\n",
            "la Defensa de así mayoría no Zedillo cosas para esto de de y celebro vive\n",
            "también puedo inundó La Pues pequeños cabo sí estructural hay a va corporaciones vuelve Hernández\n",
            "es demócratas todos iniciar y verifique todo hectáreas participativo ver y no van contra luego\n"
          ]
        }
      ],
      "source": [
        "test_perplexity4 = get_perplexity(model4, test_sents, 1)\n",
        "print(\"Unigram model perplexity on test data:\", test_perplexity4)\n",
        "print(\"\\n\"+generated_text_lindstone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "EbdgX76lmjEM",
      "metadata": {
        "id": "EbdgX76lmjEM"
      },
      "outputs": [],
      "source": [
        "model5, generated_text_lindstone1 = generate_text_lindstone(sents, 2, 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "fld9kAGxmkVY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fld9kAGxmkVY",
        "outputId": "98808b5f-c8ff-4c53-eca6-a7ddb455fed4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigram model perplexity on test data: 2.149714955328577\n",
            "\n",
            "víctimas a Pero digo Vengo acarreado porque se cobra pues todavía esa iniciativa para callar\n",
            "de que hablan de los nombres en la jefa de Chile no se obtenía pero\n",
            "lo que le va a María Teresa también respeto he sostenido de la verdadera lealtad\n",
            "<UNK> y se dejaron quiebra y cualquier plazo del Escuadrón 201 Fíjense cómo se imaginan\n",
            "que restablezcan sus boletas de Electricidad porque tengo exactamente la planta coquizadora de dos años\n",
            "que sea limitado hacerlos valer el movimiento hacia adelante preferimos <UNK> nadie se trata nada\n",
            "gobierno federal y pasan a cabo en contra de lo que dar a la delincuencia\n",
            "Pero decirle de bala porque imagínense lo mismo tribunal electoral lo que te damos una\n",
            "ahorremos 15 cuando se creó para que tiene que se va a Coatlán del martes\n",
            "y el Poder Judicial que entregaron sus hechos ocultaron toda la expropiación petrolera ellos siguen\n"
          ]
        }
      ],
      "source": [
        "test_perplexity5 = get_perplexity(model5, test_sents, 2)\n",
        "print(\"Bigram model perplexity on test data:\", test_perplexity5)\n",
        "print(\"\\n\"+generated_text_lindstone1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "amblobot",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "c0b5881aaeb643ed5e43c5646f18d04eb297c126b5ce8c1a2c7c0d7b95345209"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
